---
title: "automated_lie_detection_package demo"
format:
  html:
    code-fold: true
jupyter: python3
---

# Introduction

`automated_lie_detection_package` is a Python package for classifying statements as "truthful" or "deceptive" using machine learning models. It provides a simple API for making predictions and can be integrated into various applications.

![Streamlit App Welcome Page](images/welcome_page.png)

![Streamlit App Automated Lie Detector](images/main_page.png)

# Installation

```bash
pip3 install git+https://github.com/Programming-The-Next-Step-2025/automated-lie-detection.git
```

> **Note:**  
> Depending on your environment, you may need to use `pip` instead of `pip3`, and `python` instead of `python3`.  
> For example:
> ```bash
> pip install git+https://github.com/Programming-The-Next-Step-2025/automated-lie-detection.git
> ```


# Model Folder Setup

Before making any predictions, you need to download a pretrained model into a local `models/` directory.

```{python}
from automated_lie_detection_package import download_model_folder_from_gdrive

download_model_folder_from_gdrive()
```

![Model Download Example](images/model_download.png)

**Folder structure should look like:**
```
your_project/
├── models/
│   └── [model files here]
├── src/
│   └── automated_lie_detection_package/
│       └── ...
├── run.sh
└── ...
```

# Supported Model Types

The `modelprediction` function expects a model folder containing files for a Hugging Face Transformers **sequence classification model** (such as DistilBERT, BERT, RoBERTa, etc.) that can be loaded with `AutoTokenizer.from_pretrained` and `AutoModelForSequenceClassification.from_pretrained`.

**Requirements:**
- The model folder must include all necessary files (e.g., `config.json`, `pytorch_model.bin`, `tokenizer.json`, `vocab.txt`, etc.).
- The model should be trained or fine-tuned for sequence (text) classification tasks.

**Note:**  
Other model types (e.g., language models for text generation, token classification, etc.) are **not supported** by `modelprediction` and will result in errors.

# Usage

For a demonstration of using `modelprediction` to classify a statement, see the example below.

```{python}
from automated_lie_detection_package import modelprediction

input_text = "I swear I am innocent"

# Directly call modelprediction with just the input text
result = modelprediction(input_text)
print(result)
```

# Launching the Streamlit App

To launch the web app, run the following command from your project root (where `run.sh` is located):

```bash
./run.sh
```

This will start the Streamlit app, which you can access in your browser (default: http://localhost:8501).

# Starting the Streamlit App Without `run.sh`

If you do **not** have the `run.sh` script (for example, if you installed via `pip3` and did not clone the repo), you can start the app manually with:

```bash
python3 -m streamlit run src/automated_lie_detection_package/app.py --server.port=8501
```

> **Note:**  
> You can use `python` instead of `python3` if that's how Python is invoked on your system.  
> You can change the number after `--server.port` to any available port, as long as that port is not already in use.

# Creating `run.sh` Yourself

If you want to create the `run.sh` script for convenience, create a new file named `run.sh` in your project root with the following content:

```bash
#!/bin/bash
python3 -m streamlit run src/automated_lie_detection_package/app.py --server.port=8501
```

Or, if your environment uses `python` instead of `python3`:

```bash
#!/bin/bash
python -m streamlit run src/automated_lie_detection_package/app.py --server.port=8501
```

Then make it executable:

```bash
chmod +x run.sh
```

# Launching the Streamlit App

To launch the web app, run the following command from your project root (where `run.sh` is located):

```bash
./run.sh
```

This will start the Streamlit app, which you can access in your browser (default: http://localhost:8501).

# Starting the Streamlit App Without `run.sh`

If you do **not** have the `run.sh` script (for example, if you installed via `pip3` and did not clone the repo), you can start the app manually with:

```bash
python3 -m streamlit run path/to/app.py --server.port=8501
```

Replace `path/to/app.py` with the actual path to the app file, for example:
```bash
python3 -m streamlit run src/automated_lie_detection_package/app.py --server.port=8501
```

If needed, the number after `--server.port` can be changed to any available port, as long as that port is not already in use.

**Note**

Depending on your environment, you may need to use pip instead of pip3.
For example, on some systems, the commands would be:

```bash
python3 -m streamlit run path/to/app.py --server.port=8501
```

# Clearing the Models Folder

If you want to remove all files and subdirectories from the `models` folder (for example, before downloading a new model), use the `clear_models_folder` function:

```{python}
from automated_lie_detection_package import clear_models_folder

# This will delete all files and subfolders in the 'models' directory
clear_models_folder()
```

# Use Cases

- Automated interview analysis
- Security screening
- Academic research on deception detection
- Personal statement analysis

# Example Output

```
The statement was classified as deceptive with 87.5% confidence.
```

![Prediction Example](images/modelprediction.png)

# API Reference

- **download_model_folder_from_gdrive(gdrive_folder_url="https://drive.google.com/drive/folders/1BByWnxuJ8gXWDPEWjPjAnU4iVaeQp1dZ?usp=sharing", output_dir="models")**  
  Downloads the pretrained model from Google Drive into the specified directory.  
  If no URL is provided, the default public model folder will be used.

- **modelprediction(input_text)**  
  Classifies a statement as "truthful" or "deceptive" and returns a message with the predicted class and confidence score.

- **clear_models_folder(models_dir="models")**  
  Deletes all files and subdirectories in the specified models folder.

# FAQ

**Q:** What input does `modelprediction` expect?  
**A:** A single string statement.

**Q:** What does the function return?  
**A:** A message string describing the predicted class ("truthful" or "deceptive") and the confidence percentage.

# License

MIT License

# Credits

Developed by Lucca Pfründer.  
Built with Python, Hugging Face Transformers, and Streamlit.